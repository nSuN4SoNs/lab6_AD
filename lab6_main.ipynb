{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторна робота №6 - ФБ-31 Гек\n",
    "## Застосування numpy\n",
    "\n",
    "**Мета роботи:** отримати поглиблені навички роботи з numpy; дослідити поняття лінійної регресії та градієнтного спуску."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завдання 1.1 Генерація прямої та шуму навколо неї"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Коеф. прямої\n",
    "k = -0.7  # нахил прямої\n",
    "b = 2     # перетин з віссю y\n",
    "N = 500   # к-ть точок\n",
    "\n",
    "# Генеруємо значення x\n",
    "x = np.linspace(0, 10, N)\n",
    "\n",
    "# Генеруємо значення y\n",
    "noise = np.random.normal(0, 1.5, N)  # шум з нормальним розподілом навколо прямої\n",
    "y = k * x + b + noise\n",
    "\n",
    "# Відображення\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, label='Дані', color='purple', alpha=0.6)\n",
    "plt.plot(x, k * x + b, color='red', label='Пряма')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Дані з шумом та справжня пряма')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завдання 1.(2-4) Метод найменших квадратів\n",
    "\n",
    "\n",
    "$$b=\\frac{\\sum_{i=1}^{n}(x_i^2)\\sum_{i=1}^{n}(y_i) - \\sum_{i=1}^{n}(x_i)\\sum_{i=1}^{n}(x_i y_i)}{n\\sum_{i=1}^{n}(x_i^2)-(\\sum_{i=1}^{n}(x_i))^2}$$\n",
    "\n",
    "$$k=\\frac{n\\sum_{i=1}^{n}(x_i y_i)-(\\sum_{i=1}^{n}(x_i))(\\sum_{i=1}^{n}(y_i))}{n\\sum_{i=1}^{n}(x_i^2)-(\\sum_{i=1}^{n}(x_i))^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод найменших квадратів\n",
    "def least_squares_method(x, y):\n",
    "    n = len(x)\n",
    "    sum_x = np.sum(x)\n",
    "    sum_y = np.sum(y)\n",
    "    sum_xy = np.sum(x * y)\n",
    "    sum_xx = np.sum(x * x)\n",
    "    \n",
    "    # Коеф. прямої\n",
    "    kk_hat = (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x ** 2)\n",
    "    bb_hat = (sum_y - kk_hat * sum_x) / n\n",
    "    \n",
    "    return kk_hat, bb_hat\n",
    "\n",
    "# Оцінки коеф. за методом найменших квадратів\n",
    "kk_hat, bb_hat = least_squares_method(x, y)\n",
    "\n",
    "# Оцінки коеф. за допомогою np.polyfit\n",
    "coefficients = np.polyfit(x, y, 1)\n",
    "kk_np_polyfit = coefficients[0]\n",
    "bb_np_polyfit = coefficients[1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, label='Дані', color='purple', alpha=0.6)\n",
    "plt.plot(x, k * x + b, color='red', label='Пряма')\n",
    "plt.plot(x, kk_hat * x + bb_hat, color='green', linestyle='--', label='Оцінка методом найменших квадратів')\n",
    "plt.plot(x, kk_np_polyfit * x + bb_np_polyfit, color='blue', linestyle='-.', label='Оцінка np.polyfit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Оцінки методом найменших квадратів і np.polyfit')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Парам. справжньої прямої:\")\n",
    "print(\"k =\", k)\n",
    "print(\"b =\", b)\n",
    "print()\n",
    "print(\"Оцінки парам. за методом найменших квадратів:\")\n",
    "print(\"kk_hat =\", kk_hat)\n",
    "print(\"bb_hat =\", bb_hat)\n",
    "print()\n",
    "print(\"Оцінки парам. за np.polyfit:\")\n",
    "print(\"kk_np_polyfit =\", kk_np_polyfit)\n",
    "print(\"bb_np_polyfit =\", bb_np_polyfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завдання 2. Метод градієнтного спуску"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод градієнтного спуску використовує ітеративний підхід для знаходження мінімуму ф-ї втрат (у нашому випадку - середньоквадратичної помилки). Алгоритм починає з певних початкових значень парам. і поступово коригує їх, рухаючись у напрямку антиградієнта ф-ї втрат.\n",
    "\n",
    "Часткові похідні ф-ї втрат:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial k} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - (k x_i + b))$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - (k x_i + b))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, learning_rate, n_iter):\n",
    "\n",
    "    # Початкові оцінки коеф.\n",
    "    kk_hat = 0.0\n",
    "    bb_hat = 0.0\n",
    "    n = len(x)\n",
    "    mse_history = []\n",
    "    \n",
    "    # Ітеративний процес градієнтного спуску\n",
    "    for i in range(n_iter):\n",
    "        # Передбачені значення y\n",
    "        y_pred = kk_hat * x + bb_hat\n",
    "        \n",
    "        # Обчислення часткових похідних (градієнтів)\n",
    "        gradient_kk = (-2/n) * np.sum(x * (y - y_pred))\n",
    "        gradient_bb = (-2/n) * np.sum(y - y_pred)\n",
    "        \n",
    "        # Оновлення парам.\n",
    "        kk_hat = kk_hat - learning_rate * gradient_kk\n",
    "        bb_hat = bb_hat - learning_rate * gradient_bb\n",
    "        \n",
    "        # Обчислення середньоквадратичної помилки (MSE)\n",
    "        mse = np.mean((y - y_pred) ** 2)\n",
    "        mse_history.append(mse)\n",
    "    \n",
    "    return kk_hat, bb_hat, mse_history\n",
    "\n",
    "# Визначення оптимальних парам. градієнтного спуску\n",
    "learning_rate = 0.01\n",
    "n_iter = 1000\n",
    "\n",
    "# Виклик ф-ї градієнтного спуску\n",
    "kk_gd, bb_gd, mse_history = gradient_descent(x, y, learning_rate, n_iter)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, label='Дані', color='purple', alpha=0.6)\n",
    "plt.plot(x, k * x + b, color='red', label='Пряма')\n",
    "plt.plot(x, kk_hat * x + bb_hat, color='blue', linestyle='--', label='Оцінка методом найменших квадратів')\n",
    "plt.plot(x, kk_np_polyfit * x + bb_np_polyfit, color='green', linestyle='-.', label='Оцінка за np.polyfit')\n",
    "plt.plot(x, kk_gd * x + bb_gd, color='orange', linestyle=':', label='Оцінка градієнтним спуском')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Порівняння методів регресійного аналізу')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Парам. справжньої прямої:\")\n",
    "print(\"k =\", k)\n",
    "print(\"b =\", b)\n",
    "print()\n",
    "print(\"Оцінки парам. за методом найменших квадратів:\")\n",
    "print(\"kk_hat =\", kk_hat)\n",
    "print(\"bb_hat =\", bb_hat)\n",
    "print()\n",
    "print(\"Оцінки парам. за градієнтним спуском:\")\n",
    "print(\"kk_gd =\", kk_gd)\n",
    "print(\"bb_gd =\", bb_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дослідження впливу парам. градієнтного спуску"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Побудова графіка похибки в залежності від к-ті ітерацій\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_iter + 1), mse_history, color='blue')\n",
    "plt.xlabel('к-ть ітерацій')\n",
    "plt.ylabel('Середньоквадратична похибка (MSE)')\n",
    "plt.title('Залежність похибки від к-ті ітерацій')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вплив швидкості навчання на збіжність градієнтного спуску\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "n_iter = 1000\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for lr in learning_rates:\n",
    "    kk_gd, bb_gd, mse_history = gradient_descent(x, y, lr, n_iter)\n",
    "    plt.plot(range(1, n_iter + 1), mse_history, label=f'learning_rate = {lr}')\n",
    "    \n",
    "plt.xlabel('к-ть ітерацій')\n",
    "plt.ylabel('Середньоквадратична похибка (MSE)')\n",
    "plt.title('Вплив швидкості навчання на збіжність градієнтного спуску')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Порівняємо кінцеві результати для різних швидкостей навчання\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "n_iter = 1000\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, label='Дані', color='purple', alpha=0.3)\n",
    "plt.plot(x, k * x + b, color='red', label='Пряма')\n",
    "\n",
    "# Додаємо результати метода найменших квадратів для порівняння\n",
    "plt.plot(x, kk_hat * x + bb_hat, color='green', linestyle='--', label='Метод найменших квадратів')\n",
    "\n",
    "for lr in learning_rates:\n",
    "    kk_gd, bb_gd, _ = gradient_descent(x, y, lr, n_iter)\n",
    "    plt.plot(x, kk_gd * x + bb_gd, linestyle=':', label=f'GD, lr = {lr}')\n",
    "    \n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Результати градієнтного спуску з різними швидкостями навчання')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Висновки\n",
    "\n",
    "У цій лабораторній роботі реалізував два методи для знаходження парам. лінійної регресії:\n",
    "1. **Метод найменших квадратів (МНК)** - точне рішення за одну ітерацію\n",
    "2. **Метод градієнтного спуску** - ітеративний метод, який поступово наближається до оптимального рішення\n",
    "\n",
    "Порівняв ці методи та дослідили вплив різних парам. градієнтного спуску (швидкості навчання та к-ті ітерацій) на точність отриманих результатів.\n",
    "\n",
    "Метод найменших квадратів дає точне рішення за одну ітерацію, що є його перевагою для простих лінійних моделей\n",
    "Градієнтний спуск потребує налаштування гіперпарам. і більшої к-ті обчислень, але є більш універсальним для складніших моделей\n",
    "Занадто велика швидкість навчання може призвести до розбіжності алгоритму\n",
    "Занадто мала швидкість навчання вимагає більшої к-ті ітерацій для досягнення оптимального результату\n",
    "\n",
    "У моєму випадку обидва методи дають дуже схожі результати, що підтверджує коректність їх реалізації та ефективність для задачі лінійної регресії."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
